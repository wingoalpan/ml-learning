本transformer代码主要基于 https://mdnice.com/writing/fc0b920d4ca84837a5712df1a46865d2 的代码为基础版本，
并参考了 https://github.com/P3n9W31/transformer-pytorch 的信息 进行改造而来。

用于transformer 的学习和探索。 初步发现, 对于简单场景 (simple语料)， Multi-Heads Attention 层数设为4，翻译效果比层数6更好。

增加 mt-lstm 机器翻译模型，用于和transformer模型对比。
初步看 lstm 比 transformer有差距：
1. 训练速度远远低于transformer， 耗时是 transformer的大约 10+倍
2. 训练结果泛化能力明显弱于 transfomer

后面将进一步探索 加入dropout、调整训练轮数等， 对于翻译结果的影响 ...