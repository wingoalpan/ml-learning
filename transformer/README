本transformer代码主要基于 https://mdnice.com/writing/fc0b920d4ca84837a5712df1a46865d2 的代码为基础版本，
并参考了 https://github.com/P3n9W31/transformer-pytorch 的信息 进行改造而来。

用于transformer 的学习和探索。

一、初步发现, 对于简单场景 (simple语料)， Multi-Heads Attention 层数设为4，翻译效果比层数6更好。

增加 mt-lstm 机器翻译模型，用于和transformer模型对比。
初步看 lstm 比 transformer有差距：
1. 训练速度远远低于transformer， 耗时是 transformer的大约 10+倍
2. 训练结果泛化能力明显弱于 transfomer

后面将进一步探索 加入dropout、调整训练轮数等， 对于翻译结果的影响 ...

基于简单预料训练和翻译的对比测试结果见 benchmark/report.xlsx:
1. 训练耗时如下（1000轮）
scenario	                  |  train_secs
------------------------------+------------
lstm-simple_dropout0.0	      |  38.01418352
transformer-simple_dropout0.0 |	116.0601838
lstm-simple_dropout0.1	      |  36.48308682
transformer-simple_dropout0.1 |	 86.80951047
lstm-simple_dropout0.3	      |  52.88970089
transformer-simple_dropout0.3 |  84.09955668
lstm-simple_dropout0.4	      |  64.7153306
transformer-simple_dropout0.4 |  87.56535006

平均下来，lstm 比 transformer 快2倍左右。

2. 翻译耗时如下：
epoch |	lstm-0.0   | tf-0.0    | lstm-0.1  | tf-0.1	   | lstm-0.3  | tf-0.3    | lstm-0.4  | tf-0.4
------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------
50	    0.056000948	0.12336874	0.002000093	0.075331688	0.001999855	0.059658051	0.00400281	0.060001612
100	    0.002999544	0.062003374	0.003000021	0.071888924	0.002999544	0.047576666	0.004000664	0.065000772
200     0.002506971	0.058987141	0.003000498	0.072061777	0.002000332	0.081005335	0.002000093	0.078742504
300     0.002000093	0.059821129	0.001999617	0.085507393	0.002000332	0.054713249	0.003000021	0.082680702
400     0.002601624	0.053506374	0.003000259	0.071225643	0.001504898	0.065509796	0.004002094	0.080800772
600     0.001999855	0.062896013	0.003000259	0.076477289	0.002000332	0.059131622	0.002646923	0.084274054
800     0.001999617	0.05475378	0.003000021	0.065063953	0.00299859	0.056702852	0.003000021	0.087508202
1000    0.001999855	0.061035395	0.001999855	0.070552826	0.002999783	0.060075998	0.002999783	0.070871115

transformer翻译的平均耗时大约是 LSTM的20~30倍。

3. 翻译准确度指标(BLEU)如下：
epoch |	lstm-0.0   | tf-0.0    | lstm-0.1  | tf-0.1	   | lstm-0.3  | tf-0.3    | lstm-0.4  | tf-0.4
------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------
50     0.319993115	0.700204871	0.459736521	0.6607578	0.300175115	0.655090072	0.280364531	0.595784796
100    0.313527941	0.727971926	0.306055695	0.747281365	0.294110346	0.636876358	0.280364531	0.661675861
200    0.313527941	0.697737315	0.306055695	0.726286998	0.294110346	0.671267572	0.280364531	0.705447353
300    0.313527941	0.727971926	0.306055695	0.752442204	0.317571636	0.750811655	0.280364531	0.752772653
400    0.313527941	0.771677466	0.306055695	0.769293745	0.317571636	0.727095194	0.280364531	0.737468406
600    0.319993115	0.789304725	0.306055695	0.714820077	0.317571636	0.729823575	0.280364531	0.734740004
800    0.319993115	0.722649446	0.306055695	0.772552116	0.317571636	0.762087624	0.280364531	0.743791507
1000   0.319993115	0.705447353	0.306055695	0.770521348	0.317571636	0.762000762	0.280364531	0.759125277

transformer翻译准确度显著高于LSTM。 lstm 的翻译严格依赖源文本各单词绝对位置，对于非训练文本，翻译效果很差。
例如，对于不在训练样本中的 "我 不 喜欢 白 猫"， transformer 和 lstm 的翻译结果分别是：
transformer翻译结果："don't"
       lstm翻译结果："I don't like white cat ."
       

二、 支持基于LORA的Adapter模式，验证情况如下：

在 复杂语料 TP3n9W31Data 800轮预训练模型的基础上， 叠加 SimpleData 语料的Adapter训练（500轮）。
训练后的Adapter模型，验证SimpleData语料，结果符合预期。 但是附加Adapter的模型，对于原TP3n9W31Data 语料，验证结果几乎100失败。

因此这里我有个疑问待解：在通用大模型基础上，基于LORA机制补充训练定制场景的模型， 这个补充训练后的模型似乎扰乱了原来通用模型 对通用场景的预测能力，
如何做到模型 在保证原来通用场景预测质量不下降的情况下，有支持定制场景的预测？

待进一步探究 。。。。